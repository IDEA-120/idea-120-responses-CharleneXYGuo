Bridle's argument that the internet is not a safe place for children to freely roam is backed by ample evidence. I share this concern as I recall my own experiences of accessing the internet when I was growing up. At that time, there was little knowledge about appropriate online content for children, and some of the content I came across was definitely not suitable for my age group. Unfortunately, this type of content was normalized, and there was no regulatory framework in place to protect children.

While I agree with Bridle that there is now an epidemic of harmful and exploitative content being marketed to children, I think it's important to acknowledge that this has been an issue for a long time, but in different forms. Creators used to rely on bright, flashy colors and bold text to draw attention to their content, even if it was sexual or vulgar in nature.

However, the difference now is that the proliferation of iPads and other digital devices has made it much easier for algorithms to generate disturbing content alongside the actual content aimed at children. This is because children can easily access the internet on their own, clicking endlessly and providing more data for algorithms to exploit.

While media platforms do need to take responsibility, I recognize that this is a challenging task. To be effective, content moderation would need to occur at the level of television networks that produce shows for children. The current YouTube Kids platform is a step in the right direction, but I've heard it's not entirely reliable. A more effective solution would be to have a team of people approve each video on a case-by-case basis. But even then, this wouldn't fully resolve the issue. Parents also need to monitor their children's internet usage and limit their access to inappropriate content. Even with filtered content on platforms like YouTube Kids, children can still search for other content on different platforms.
